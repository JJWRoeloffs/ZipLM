import sys
import json
import time
import random
import argparse
from pathlib import Path
from pprint import pprint
from dataclasses import dataclass

import torch
import torch.nn.functional as F
from transformers import T5Tokenizer, T5ForConditionalGeneration

from zip_lm.eval import evaluate, get_testdata_subset
from zip_lm.readdata import BlimpPyItem, get_blimp_data


from typing import List, Literal


@dataclass
class Args:
    seed: int
    nr_per_type: int
    model_name: Literal["t5-small", "t5-base", "t5-large", "t5-3b", "t5-11b"]


def parse_args(args: List[str]) -> Args:
    parser = argparse.ArgumentParser(
        prog="evaluate_t5",
        description="The script to use to evaluate a pretrained t5 model",
    )
    parser.add_argument("seed", type=int, help="The seed to use for the rng")
    parser.add_argument(
        "nr_per_type", type=int, help="The amount of blimp items to get per type"
    )
    parser.add_argument(
        "--model_name",
        choices=["t5-small", "t5-base", "t5-large", "t5-3b", "t5-11b"],
        default="t5-small",
        nargs="?",
        help="The model to run",
    )
    arguments = parser.parse_args(args)
    return Args(arguments.seed, arguments.nr_per_type, arguments.model_name)


def calculate_log_likelihoods(
    items: List[BlimpPyItem], model: T5ForConditionalGeneration, tokenizer: T5Tokenizer
):
    cola_labels = ["acceptable", "unacceptable"]

    # Taken from https://discuss.huggingface.co/t/compute-log-probabilities-of-any-sequence-provided/11710/3
    def get_scores_for_labels(input, labels, model, tokenizer):
        batch_size, num_labels = len(input), len(labels)
        input_enc = tokenizer.batch_encode_plus(
            input,
            return_tensors="pt",
            add_special_tokens=True,
            truncation=True,
            padding="longest",
        )
        target_enc = tokenizer.batch_encode_plus(
            labels, return_tensors="pt", padding="longest"
        ).input_ids

        # Get encoder's last hidden state
        encoder_hidden_states = model.encoder(**input_enc)[0]

        # Repeat the inputs `num_label` times
        encoder_hidden_states = (
            encoder_hidden_states.unsqueeze(dim=1)
            .repeat(1, num_labels, 1, 1)
            .flatten(0, 1)
        )
        attention_mask = (
            input_enc.attention_mask.unsqueeze(dim=1)
            .repeat(1, num_labels, 1)
            .flatten(0, 1)
        )

        # Create the decoding mask (that is commonly generated by the T5 model at predict time) -- makes it more efficient
        decoder_input_ids = torch.cat(
            [
                torch.zeros((num_labels * batch_size, 1), dtype=torch.int),
                target_enc[:, :-1].repeat(num_labels, 1),
            ],
            dim=1,
        )
        decoder_attention_mask = (decoder_input_ids == decoder_input_ids).float()
        lm_target = target_enc - 100 * (target_enc == tokenizer.pad_token_id).long()

        model_output = model(
            attention_mask=attention_mask,
            encoder_outputs=[encoder_hidden_states],
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
        )

        # Compute the log probabilities associated with each of the labels
        labels_log_probs = F.cross_entropy(
            model_output.logits.flatten(0, 1),
            lm_target.repeat(num_labels, 1).flatten(0, 1),
            reduction="none",
        )

        # Sum log probs for each of the (input, label) pair
        labels_scores = labels_log_probs.view(batch_size, num_labels, -1)
        labels_scores = labels_scores.sum(dim=-1)

        # Note: Label log probabilities are positive (due to the internals of pytorch's
        # cross entropy). To obtain the "logits", we need to multiply by -1.
        return labels_scores * -1

    model.eval()

    with torch.no_grad():
        for item in items:
            probs = get_scores_for_labels(
                [item.sentence_good, item.sentence_bad], cola_labels, model, tokenizer
            ).exp()
            # This is the most jank I'm willing to accept, but it works.
            # Despite the fields being called "ll_" passing the sum of normal
            # probabilities still leaves the evaluation code valid, despite the
            # values themselves meaning absolutely nothing
            # The first value should be higher than the second. That's all we care about.
            item.ll_sentence_good = probs[0][0].item() + probs[1][1].item()
            item.ll_sentence_bad = probs[1][0].item() + probs[0][1].item()


def run(args: Args) -> None:
    before = time.time()
    random.seed(args.seed)
    tokenizer = T5Tokenizer.from_pretrained(args.model_name)
    model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(
        args.model_name
    )  # type: ignore

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)  # type: ignore
    print(f"Retrieved {args.model_name} in {time.time() - before:.2f} seconds")

    before = time.time()
    testdata = get_blimp_data("blimp/data")
    inputdata = get_testdata_subset(testdata, args.nr_per_type)
    print(f"Retrieved testing data in {time.time() - before:.2f} seconds")

    before = time.time()
    calculate_log_likelihoods(inputdata, model, tokenizer)
    print(f"Calculated lls with {args.model_name} in {time.time()-before:.2f} seconds")

    results = {
        "seed": args.seed,
        "type": args.model_name,
        "evaluation_subset": args.nr_per_type,
        "results": evaluate(inputdata),
    }
    pprint(results)

    results_dir = Path() / "results"
    results_dir.mkdir(exist_ok=True)

    results_file = results_dir / "t5model.jsonl"
    with results_file.open("a", encoding="utf-8") as f:
        json.dump(results, f)
        f.write("\n")


def main(args: List[str]) -> None:
    run(parse_args(args))


if __name__ == "__main__":
    main(sys.argv[1:])
